# -*- coding: utf-8 -*-
"""classification1 after

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15aYaVgXctaZUX_BnhpzaXhKNXQsRJjrL
"""

# -*- coding: utf-8 -*-
"""Final CS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xwlBN4zBlBilJ0hWsCIOB_vQRBtvF4vZ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn as sk
from sklearn.linear_model import Ridge , Lasso
from sklearn.ensemble import RandomForestRegressor , GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.pipeline import make_pipeline
from sklearn.tree import DecisionTreeRegressor
from sklearn.feature_selection import mutual_info_classif
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor
import pickle
from sklearn.metrics import accuracy_score

data = pd.read_csv(r"/content/ElecDeviceRatingPrediction_Milestone2(in).csv")

data.shape


data['Touchscreen'] = data['Touchscreen'].replace({'Yes': 1, 'No': 0})
data['msoffice'] = data['msoffice'].replace({'Yes': 1, 'No': 0})
data['warranty'] = data['warranty'].replace('No warranty', 0)
data['rating'] = data['rating'].replace('Bad Rating', 0)
data['rating'] = data['rating'].replace('Good Rating', 1)
data['processor_gnrtn'] = data['processor_gnrtn'].replace('Not Available', 0)

preprocessing_steps = {
    'Touchscreen': {'Yes': 1, 'No': 0},
    'msoffice': {'Yes': 1, 'No': 0},
    'warranty': {'No warranty': 0},
    'rating': {'Bad Rating': 0, 'Good Rating': 1},
    'processor_gnrtn': {'Not Available': 0},
}

with open('preprocessing_steps.pkl', 'wb') as f:
    pickle.dump(preprocessing_steps, f)


features = data.copy()

modes = features.mode().iloc[0]

for column in features.columns:
    features[column].fillna(features[column].mode()[0], inplace=True)
with open('modes.pkl', 'wb') as f:
    pickle.dump(modes, f)

import pandas as pd
import pickle
import re

columns_to_process = ['ram_gb', 'ssd', 'hdd', 'os', 'graphic_card_gb', 'warranty','processor_gnrtn']






number_regex = r'\d+\.?\d*'

for column in columns_to_process:
    if column in features.columns:
        original_values = features[column].unique()
        split_values = [re.search(number_regex, value).group() if re.search(number_regex, value) else value for value in original_values.astype(str)]


        features[column] = features[column].astype(str).apply(lambda x: re.search(number_regex, x).group() if re.search(number_regex, x) else x)

        features[column] = pd.to_numeric(features[column], errors='coerce')







features.isnull().sum()

features.head()

from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder()
features_encoded = encoder.fit_transform(features[['weight','processor_name','processor_brand','brand','ram_type']])

features_encoded_df = pd.DataFrame(features_encoded.toarray(), columns=encoder.get_feature_names_out(['weight','processor_name','processor_brand','brand','ram_type']))

features = features.drop(['weight', 'processor_name', 'processor_brand', 'brand', 'ram_type'], axis=1)

features = pd.concat([features, features_encoded_df], axis=1)

with open('encoder.pkl', 'wb') as f:
    pickle.dump(encoder, f)


import pickle

import pickle

numerical_preprocessing = {}
numerical_columns = features.select_dtypes(include=[ 'int64']).columns

for column in numerical_columns:
    median = features[column].median()
    Q1 = features[column].quantile(0.25)
    Q3 = features[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    features[column] = features[column].clip(lower=lower_bound, upper=upper_bound)
    numerical_preprocessing[column] = {'median': median, 'lower_bound': lower_bound, 'upper_bound': upper_bound}

with open('numerical_preprocessing.pkl', 'wb') as f:
    pickle.dump(numerical_preprocessing, f)

features.head()

features.head()

import pickle
from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()

non_numeric_columns = features.select_dtypes(exclude=['number']).columns

features_numeric = features.drop(columns=non_numeric_columns)

features_numeric = features_numeric.apply(pd.to_numeric, errors='coerce')


df_test_without_rating = features_numeric.drop(columns=['rating'])

robust_scaled_data = scaler.fit_transform(df_test_without_rating)

df_scaled = pd.DataFrame(robust_scaled_data, columns=df_test_without_rating.columns)

df_scaled['rating'] = features_numeric['rating']



with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler,f)

features=df_scaled

features.columns

features.head()

features['rating'].value_counts()

numeric_columns = features.select_dtypes(include=['int64', 'float64']).columns
selected_columns = features[numeric_columns]
correlation_matrix = selected_columns.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix with Ratings')
#plt.show()
strong_correlations = correlation_matrix.index[(correlation_matrix['rating'].abs() >= 0.04) & (correlation_matrix['rating'].abs() < 1)]
numeric_columns = features.select_dtypes(include=['float64', 'int64']).columns
features_numeric = features[numeric_columns]
features_numeric_filtered = features_numeric[strong_correlations]
columns_to_drop = [col for col in features_numeric if col not in features_numeric_filtered]
features = features.drop(columns=columns_to_drop, axis=1)




with open('top_features.pkl', 'wb') as f:
    pickle.dump(columns_to_drop, f)

features.columns

import pickle
import pandas as pd

def preprocess_data(df):
    for column in df.columns:
        if df[column].dtype == bool:
            print("flag")
            df[column] = df[column].astype(int)

    return df


with open('preprocessing_bool.pkl', 'wb') as f:
    pickle.dump(preprocess_data(features), f)

features.head()

import pickle
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

import time
target=data['rating']
x=features
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

param_lr = {'C': 0.0001, 'penalty': 'l2'}
param_rf = {'max_depth': 9, 'n_estimators': 25}
param_gb = {'learning_rate': 0.1, 'n_estimators': 50}
param_knn = {'n_neighbors': 3, 'weights': 'uniform'}


lr = LogisticRegression(**param_lr, solver='liblinear')
rf = RandomForestClassifier(**param_rf)
gb = GradientBoostingClassifier(**param_gb)

start_train_lr = time.time()
lr.fit(X_train, y_train)
end_train_lr = time.time()
total_train_time_lr = end_train_lr - start_train_lr

start_train_rf = time.time()
rf.fit(X_train, y_train)
end_train_rf = time.time()
total_train_time_rf = end_train_rf - start_train_rf

start_train_gb = time.time()
gb.fit(X_train, y_train)
end_train_gb = time.time()
total_train_time_gb = end_train_gb - start_train_gb




with open('lr_model.pkl', 'wb') as f:
    pickle.dump(lr, f)

with open('rf_model.pkl', 'wb') as f:
    pickle.dump(rf, f)

with open('gb_model.pkl', 'wb') as f:
    pickle.dump(gb, f)


def print_results(model, model_name, X_test, y_test):
    print(f"Results for {model_name}:")
    train_accuracy = model.score(X_train, y_train)
    test_accuracy = model.score(X_test, y_test)
    cv_error = 1 - test_accuracy
    print(f"Train Accuracy: {train_accuracy:.3f}, Test Accuracy: {test_accuracy:.3f}, CV Error: {cv_error:.3f}")

start_test_lr = time.time()
print_results(lr, "Logistic Regression", X_test, y_test)
end_test_lr = time.time()
total_test_time_lr = end_test_lr - start_test_lr

start_test_rf = time.time()
print_results(rf, "Random Forest", X_test, y_test)
end_test_rf = time.time()
total_test_time_rf = end_test_rf - start_test_rf

start_test_gb = time.time()
print_results(gb, "Gradient Boosting", X_test, y_test)
end_test_gb = time.time()
total_test_time_gb = end_test_gb - start_test_gb



print("\nTotal time for testing each model:")
print(f"Logistic Regression: {total_test_time_lr:.3f} seconds")
print(f"Random Forest: {total_test_time_rf:.3f} seconds")
print(f"Gradient Boosting: {total_test_time_gb:.3f} seconds")

# print_results(lr, "Logistic Regression", X_test, y_test)
# print_results(rf, "Random Forest", X_test, y_test)
# print_results(gb, "Gradient Boosting", X_test, y_test)
# print_results(knn, "K-Nearest Neighbors (KNN)", X_test, y_test)

from sklearn.ensemble import VotingClassifier


voting_clf = VotingClassifier(estimators=[
    ('gb', gb),
    ('rf', rf),
    ('lr', lr)
], voting='hard')


voting_clf.fit(X_train, y_train)

voting_accuracy = accuracy_score(y_test, voting_clf.predict(X_test))
print("Voting Ensemble Testing Accuracy:", voting_accuracy * 100, '%')



with open('voting_clf_model.pkl', 'wb') as f:
    pickle.dump(voting_clf, f)

from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression


model_Log = LogisticRegression()

estimators = [
    ('gb', gb),
    ('rf', rf),
    ('lr', lr)
]

stacking_clf = StackingClassifier(estimators=estimators, final_estimator=model_Log)

stacking_clf.fit(X_train, y_train)

stacking_accuracy = accuracy_score(y_test, stacking_clf.predict(X_test))
print("Stacking Ensemble Testing Accuracy:", stacking_accuracy * 100, '%')

with open('stacking_model.pkl', 'wb') as f:
    pickle.dump(stacking_clf, f)